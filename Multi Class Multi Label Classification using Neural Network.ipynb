{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\thinkpad\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#assume a classification problem with 5 different labels\n",
    "#set up a simple neural net with 5 output nodes, one output node for each possible class.\n",
    "nn = Sequential()\n",
    "nn.add(Dense(10, activation=\"relu\", input_shape=(10,)))\n",
    "nn.add(Dense(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The usual choice for multi-class classification is the softmax layer. The softmax function is a generalization of the logistic function that “squashes” a K-dimensional vector \\mathbf{z} of arbitrary real values to a K-dimensional vector \\sigma(\\mathbf{z}) of real values in the range [0, 1] that add up to 1.\n",
    "import math\n",
    "\n",
    "def softmax(z):\n",
    "    z_exp = [math.exp(i) for i in z]\n",
    "    sum_z_exp = sum(z_exp)\n",
    "    return [i / sum_z_exp for i in z_exp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.031062774127550943,\n",
       " 0.0844373744524495,\n",
       " 0.22952458061688552,\n",
       " 0.623912496675563,\n",
       " 0.031062774127550943]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multi Class Classification\n",
    "#Assume our last layer (before the activation) returns the numbers z = [1.0, 2.0, 3.0, 4.0, 1.0]. Every number is the value for a class.\n",
    "z = [1.0, 2.0, 3.0, 4.0, 1.0]\n",
    "softmax(z)\n",
    "#Result says->we would predict class 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0014152405960574873,\n",
       " 0.5709488061694115,\n",
       " 0.002333337273878307,\n",
       " 0.4229692786867745,\n",
       " 0.002333337273878307]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Multi Class Multi Label Classification\n",
    "z = [-1.0, 5.0, -0.5, 4.7, -0.5]\n",
    "softmax(z)\n",
    "#Result says -> using softmax, we would clearly pick class 2 and 4. But we have to know how many labels we want for a sample or have to pick a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A common activation function for binary classification is the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return [1 / (1 + math.exp(-n)) for n in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2689414213699951,\n",
       " 0.9933071490757153,\n",
       " 0.3775406687981454,\n",
       " 0.9933071490757153,\n",
       " 0.3775406687981454]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = [-1.0, 5.0, -0.5, 5.0, -0.5]\n",
    "sigmoid(z)\n",
    "#Result says -> Now the probabilities of each class is independent from the other class probabilities. So we can use the threshold 0.5 as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Dense(10, activation=\"relu\", input_shape=(10,)))\n",
    "nn.add(Dense(5, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the binary_crossentropy loss and not the usual in multi-class classification used categorical_crossentropy loss.\n",
    "#we pick a binary loss and model the output of the network as a independent Bernoulli distributions per label.\n",
    "nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
