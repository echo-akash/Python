{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_and_target(dict_keys_time, seq_len=50):\n",
    "    \"\"\" Generate input and the target of our deep learning for one music.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dict_keys_time : dict\n",
    "      Dictionary of timestep and notes\n",
    "    seq_len : int\n",
    "      The length of the sequence\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    Tuple of list of input and list of target of neural network.\n",
    "    \n",
    "       \n",
    "    \"\"\"\n",
    "    # Get the start time and end time\n",
    "    start_time, end_time = list(dict_keys_time.keys())[0], list(dict_keys_time.keys())[-1]\n",
    "    list_training, list_target = [], []\n",
    "    for index_enum, time in enumerate(range(start_time, end_time)):\n",
    "        list_append_training, list_append_target = [], []\n",
    "        start_iterate = 0\n",
    "        flag_target_append = False # flag to append the test list\n",
    "        if index_enum < seq_len:\n",
    "            start_iterate = seq_len - index_enum - 1\n",
    "            for i in range(start_iterate): # add 'e' to the seq list. \n",
    "                list_append_training.append('e')\n",
    "                flag_target_append = True\n",
    "\n",
    "        for i in range(start_iterate,seq_len):\n",
    "            index_enum = time - (seq_len - i - 1)\n",
    "            if index_enum in dict_keys_time:\n",
    "                list_append_training.append(','.join(str(x) for x in dict_keys_time[index_enum]))      \n",
    "            else:\n",
    "                list_append_training.append('e')\n",
    "\n",
    "        # add time + 1 to the list_append_target\n",
    "        if time+1 in dict_keys_time:\n",
    "            list_append_target.append(','.join(str(x) for x in dict_keys_time[time+1]))\n",
    "        else:\n",
    "            list_append_target.append('e')\n",
    "        list_training.append(list_append_training)\n",
    "        list_target.append(list_append_target)\n",
    "    return list_training, list_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoteTokenizer:\n",
    "    \n",
    "    def __init__(self):\n",
    "      self.notes_to_index = {}\n",
    "      self.index_to_notes = {}\n",
    "      self.num_of_word = 0\n",
    "      self.unique_word = 0\n",
    "      self.notes_freq = {}\n",
    "        \n",
    "    def transform(self,list_array):\n",
    "      \"\"\" Transform a list of note in string into index.\n",
    "      \n",
    "      Parameters\n",
    "      ==========\n",
    "      list_array : list\n",
    "        list of note in string format\n",
    "      \n",
    "      Returns\n",
    "      =======\n",
    "      The transformed list in numpy array.\n",
    "      \n",
    "      \"\"\"\n",
    "      transformed_list = []\n",
    "      for instance in list_array:\n",
    "          transformed_list.append([self.notes_to_index[note] for note in instance])\n",
    "      return np.array(transformed_list, dtype=np.int32)\n",
    " \n",
    "    def partial_fit(self, notes):\n",
    "        \"\"\" Partial fit on the dictionary of the tokenizer\n",
    "        \n",
    "        Parameters\n",
    "        ==========\n",
    "        notes : list of notes\n",
    "        \n",
    "        \"\"\"\n",
    "        for note in notes:\n",
    "            note_str = ','.join(str(a) for a in note)\n",
    "            if note_str in self.notes_freq:\n",
    "                self.notes_freq[note_str] += 1\n",
    "                self.num_of_word += 1\n",
    "            else:\n",
    "                self.notes_freq[note_str] = 1\n",
    "                self.unique_word += 1\n",
    "                self.num_of_word += 1\n",
    "                self.notes_to_index[note_str], self.index_to_notes[self.unique_word] = self.unique_word, note_str\n",
    "            \n",
    "    def add_new_note(self, note):\n",
    "        \"\"\" Add a new note into the dictionary\n",
    "        Parameters\n",
    "        ==========\n",
    "        note : str\n",
    "          a new note who is not in dictionary.  \n",
    "        \"\"\"\n",
    "        assert note not in self.notes_to_index\n",
    "        self.unique_word += 1\n",
    "        self.notes_to_index[note], self.index_to_notes[self.unique_word] = self.unique_word, note\n",
    "        \n",
    "def generate_batch_song(list_all_midi, batch_music=16, start_index=0, fs=30, seq_len=50, use_tqdm=False):\n",
    "    \"\"\"\n",
    "    Generate Batch music that will be used to be input and output of the neural network\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    list_all_midi : list\n",
    "      List of midi files\n",
    "    batch_music : int\n",
    "      A number of music in one batch\n",
    "    start_index : int\n",
    "      The start index to be batched in list_all_midi\n",
    "    fs : int\n",
    "      Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    seq_len : int\n",
    "      The sequence length of the music to be input of neural network\n",
    "    use_tqdm : bool\n",
    "      Whether to use tqdm or not in the function\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    Tuple of input and target neural network\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(list_all_midi) >= batch_music\n",
    "    dict_time_notes = generate_dict_time_notes(list_all_midi, batch_music, start_index, fs, use_tqdm=use_tqdm)\n",
    "    \n",
    "    list_musics = process_notes_in_song(dict_time_notes, seq_len)\n",
    "    collected_list_input, collected_list_target = [], []\n",
    "     \n",
    "    for music in list_musics:\n",
    "        list_training, list_target = generate_input_and_target(music, seq_len)\n",
    "        collected_list_input += list_training\n",
    "        collected_list_target += list_target\n",
    "    return collected_list_input, collected_list_target\n",
    "\n",
    "def generate_dict_time_notes(list_all_midi, batch_song = 16, start_index=0, fs=30, use_tqdm=True):\n",
    "    \"\"\" Generate map (dictionary) of music ( in index ) to piano_roll (in np.array)\n",
    "    Parameters\n",
    "    ==========\n",
    "    list_all_midi : list\n",
    "        List of midi files\n",
    "    batch_music : int\n",
    "      A number of music in one batch\n",
    "    start_index : int\n",
    "      The start index to be batched in list_all_midi\n",
    "    fs : int\n",
    "      Sampling frequency of the columns, i.e. each column is spaced apart\n",
    "        by ``1./fs`` seconds.\n",
    "    use_tqdm : bool\n",
    "      Whether to use tqdm or not in the function\n",
    "    Returns\n",
    "    =======\n",
    "    dictionary of music to piano_roll (in np.array)\n",
    "    \"\"\"\n",
    "    assert len(list_all_midi) >= batch_song\n",
    "    \n",
    "    dict_time_notes = {}\n",
    "    process_tqdm_midi = tqdm_notebook(range(start_index, min(start_index + batch_song, len(list_all_midi)))) if use_tqdm else range(start_index,  min(start_index + batch_song, len(list_all_midi)))\n",
    "    for i in process_tqdm_midi:\n",
    "        midi_file_name = list_all_midi[i]\n",
    "        if use_tqdm:\n",
    "            process_tqdm_midi.set_description(\"Processing {}\".format(midi_file_name))\n",
    "        try: # Handle exception on malformat MIDI files\n",
    "            midi_pretty_format = pretty_midi.PrettyMIDI(midi_file_name)\n",
    "            piano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n",
    "            piano_roll = piano_midi.get_piano_roll(fs=fs)\n",
    "            dict_time_notes[i] = piano_roll\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"broken file : {}\".format(midi_file_name))\n",
    "            pass\n",
    "    return dict_time_notes\n",
    "\n",
    "def generate_input_and_target(dict_keys_time, seq_len=50):\n",
    "    \"\"\" Generate input and the target of our deep learning for one music.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dict_keys_time : dict\n",
    "      Dictionary of timestep and notes\n",
    "    seq_len : int\n",
    "      The length of the sequence\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    Tuple of list of input and list of target of neural network.\n",
    "    \n",
    "       \n",
    "    \"\"\"\n",
    "    # Get the start time and end time\n",
    "    start_time, end_time = list(dict_keys_time.keys())[0], list(dict_keys_time.keys())[-1]\n",
    "    list_training, list_target = [], []\n",
    "    for index_enum, time in enumerate(range(start_time, end_time)):\n",
    "        list_append_training, list_append_target = [], []\n",
    "        start_iterate = 0\n",
    "        flag_target_append = False # flag to append the test list\n",
    "        if index_enum < seq_len:\n",
    "            start_iterate = seq_len - index_enum - 1\n",
    "            for i in range(start_iterate): # add 'e' to the seq list. \n",
    "                list_append_training.append('e')\n",
    "                flag_target_append = True\n",
    "\n",
    "        for i in range(start_iterate,seq_len):\n",
    "            index_enum = time - (seq_len - i - 1)\n",
    "            if index_enum in dict_keys_time:\n",
    "                list_append_training.append(','.join(str(x) for x in dict_keys_time[index_enum]))      \n",
    "            else:\n",
    "                list_append_training.append('e')\n",
    "\n",
    "        # add time + 1 to the list_append_target\n",
    "        if time+1 in dict_keys_time:\n",
    "            list_append_target.append(','.join(str(x) for x in dict_keys_time[time+1]))\n",
    "        else:\n",
    "            list_append_target.append('e')\n",
    "        list_training.append(list_append_training)\n",
    "        list_target.append(list_append_target)\n",
    "    return list_training, list_target\n",
    "\n",
    "def process_notes_in_song(dict_time_notes, seq_len = 50):\n",
    "    \"\"\"\n",
    "    Iterate the dict of piano rolls into dictionary of timesteps and note played\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    dict_time_notes : dict\n",
    "      dict contains index of music ( in index ) to piano_roll (in np.array)\n",
    "    seq_len : int\n",
    "      Length of the sequence\n",
    "      \n",
    "    Returns\n",
    "    =======\n",
    "    Dict of timesteps and note played\n",
    "    \"\"\"\n",
    "    list_of_dict_keys_time = []\n",
    "    \n",
    "    for key in dict_time_notes:\n",
    "        sample = dict_time_notes[key]\n",
    "        times = np.unique(np.where(sample > 0)[1])\n",
    "        index = np.where(sample > 0)\n",
    "        dict_keys_time = {}\n",
    "\n",
    "        for time in times:\n",
    "            index_where = np.where(index[1] == time)\n",
    "            notes = index[0][index_where]\n",
    "            dict_keys_time[time] = notes\n",
    "        list_of_dict_keys_time.append(dict_keys_time)\n",
    "    return list_of_dict_keys_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq_len, unique_notes, dropout=0.3, output_emb=100, rnn_unit=128, dense_unit=64):\n",
    "  inputs = tf.keras.layers.Input(shape=(seq_len,))\n",
    "  embedding = tf.keras.layers.Embedding(input_dim=unique_notes+1, output_dim=output_emb, input_length=seq_len)(inputs)\n",
    "  forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit, return_sequences=True))(embedding)\n",
    "  forward_pass , att_vector = SeqSelfAttention(\n",
    "      return_attention=True,\n",
    "      attention_activation='sigmoid', \n",
    "      attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "      attention_width=50, \n",
    "      kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "      bias_regularizer=tf.keras.regularizers.l1(1e-4),\n",
    "      attention_regularizer_weight=1e-4,\n",
    "  )(forward_pass)\n",
    "  forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "  forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit, return_sequences=True))(forward_pass)\n",
    "  forward_pass , att_vector2 = SeqSelfAttention(\n",
    "      return_attention=True,\n",
    "      attention_activation='sigmoid', \n",
    "      attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL,\n",
    "      attention_width=50, \n",
    "      kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "      bias_regularizer=tf.keras.regularizers.l1(1e-4),\n",
    "      attention_regularizer_weight=1e-4,\n",
    "  )(forward_pass)\n",
    "  forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "  forward_pass = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(rnn_unit))(forward_pass)\n",
    "  forward_pass = tf.keras.layers.Dropout(dropout)(forward_pass)\n",
    "  forward_pass = tf.keras.layers.Dense(dense_unit)(forward_pass)\n",
    "  forward_pass = tf.keras.layers.LeakyReLU()(forward_pass)\n",
    "  outputs = tf.keras.layers.Dense(unique_notes+1, activation = \"softmax\")(forward_pass)\n",
    "\n",
    "  model = tf.keras.Model(inputs=inputs, outputs=outputs, name='generate_scores_rnn')\n",
    "  return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel:\n",
    "  \n",
    "  def __init__(self, epochs, note_tokenizer, sampled_200_midi, frame_per_second, \n",
    "               batch_nnet_size, batch_song, optimizer, checkpoint, loss_fn,\n",
    "               checkpoint_prefix, total_songs, model):\n",
    "    self.epochs = epochs\n",
    "    self.note_tokenizer = note_tokenizer\n",
    "    self.sampled_200_midi = sampled_200_midi\n",
    "    self.frame_per_second = frame_per_second\n",
    "    self.batch_nnet_size = batch_nnet_size\n",
    "    self.batch_song = batch_song\n",
    "    self.optimizer = optimizer\n",
    "    self.checkpoint = checkpoint\n",
    "    self.loss_fn = loss_fn\n",
    "    self.checkpoint_prefix = checkpoint_prefix\n",
    "    self.total_songs = total_songs\n",
    "    self.model = model\n",
    "    \n",
    "  def train(self):\n",
    "    for epoch in tqdm_notebook(range(self.epochs),desc='epochs'):\n",
    "      # for each epochs, we shufle the list of all the datasets\n",
    "      shuffle(self.sampled_200_midi)\n",
    "      loss_total = 0\n",
    "      steps = 0\n",
    "      steps_nnet = 0\n",
    "\n",
    "      # We will iterate all songs by self.song_size\n",
    "      for i in tqdm_notebook(range(0,self.total_songs, self.batch_song), desc='MUSIC'):\n",
    "\n",
    "        steps += 1\n",
    "        inputs_nnet_large, outputs_nnet_large = generate_batch_song(\n",
    "            self.sampled_200_midi, self.batch_song, start_index=i, fs=self.frame_per_second, \n",
    "            seq_len=seq_len, use_tqdm=False) # We use the function that have been defined here\n",
    "        inputs_nnet_large = np.array(self.note_tokenizer.transform(inputs_nnet_large), dtype=np.int32)\n",
    "        outputs_nnet_large = np.array(self.note_tokenizer.transform(outputs_nnet_large), dtype=np.int32)\n",
    "\n",
    "        index_shuffled = np.arange(start=0, stop=len(inputs_nnet_large))\n",
    "        np.random.shuffle(index_shuffled)\n",
    "\n",
    "        for nnet_steps in tqdm_notebook(range(0,len(index_shuffled),self.batch_nnet_size)):\n",
    "          steps_nnet += 1\n",
    "          current_index = index_shuffled[nnet_steps:nnet_steps+self.batch_nnet_size]\n",
    "          inputs_nnet, outputs_nnet = inputs_nnet_large[current_index], outputs_nnet_large[current_index]\n",
    "          \n",
    "          # To make sure no exception thrown by tensorflow on autograph\n",
    "          if len(inputs_nnet) // self.batch_nnet_size != 1:\n",
    "            break\n",
    "          loss = self.train_step(inputs_nnet, outputs_nnet)\n",
    "          loss_total += tf.math.reduce_sum(loss)\n",
    "          if steps_nnet % 20 == 0:\n",
    "            print(\"epochs {} | Steps {} | total loss : {}\".format(epoch + 1, steps_nnet, loss_total))\n",
    "\n",
    "      checkpoint.save(file_prefix = self.checkpoint_prefix)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_random(unique_notes, seq_len=50):\n",
    "  generate = np.random.randint(0,unique_notes,seq_len).tolist()\n",
    "  return generate\n",
    "    \n",
    "def generate_from_one_note(note_tokenizer, new_notes='35'):\n",
    "  generate = [note_tokenizer.notes_to_index['e'] for i in range(49)]\n",
    "  generate += [note_tokenizer.notes_to_index[new_notes]]\n",
    "  return generate\n",
    "\n",
    "def generate_notes(generate, model, unique_notes, max_generated=1000, seq_len=50):\n",
    "  for i in tqdm_notebook(range(max_generated), desc='genrt'):\n",
    "    test_input = np.array([generate])[:,i:i+seq_len]\n",
    "    predicted_note = model.predict(test_input)\n",
    "    random_note_pred = choice(unique_notes+1, 1, replace=False, p=predicted_note[0])\n",
    "    generate.append(random_note_pred[0])\n",
    "  return generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_midi_file_from_generated(generate, midi_file_name = \"result.mid\", start_index=49, fs=8, max_generated=1000):\n",
    "  note_string = [note_tokenizer.index_to_notes[ind_note] for ind_note in generate]\n",
    "  array_piano_roll = np.zeros((128,max_generated+1), dtype=np.int16)\n",
    "  for index, note in enumerate(note_string[start_index:]):\n",
    "    if note == 'e':\n",
    "      pass\n",
    "    else:\n",
    "      splitted_note = note.split(',')\n",
    "      for j in splitted_note:\n",
    "        array_piano_roll[int(j),index] = 1\n",
    "  generate_to_midi = piano_roll_to_pretty_midi(array_piano_roll, fs=fs)\n",
    "  print(\"Tempo {}\".format(generate_to_midi.estimate_tempo()))\n",
    "  for note in generate_to_midi.instruments[0].notes:\n",
    "    note.velocity = 100\n",
    "  generate_to_midi.write(midi_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
